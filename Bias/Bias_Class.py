# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v3IbxbVD7hFxZjNCmhFKEEWnRraCMrI2
"""

!pip install aif360
!pip install BlackBoxAuditing
!pip install fairlearn

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from aif360.datasets import StandardDataset
from aif360.algorithms.preprocessing import Reweighing
from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric

class BiasAwareHiringModel:
    def __init__(self, seed=42):
        self.seed = seed
        self.model = None
        self.scaler = None
        self.privileged_groups = [{'Gender': 1}]
        self.unprivileged_groups = [{'Gender': 0}]
        np.random.seed(seed)

    def generate_data(self, n_samples=100):
        """Generate synthetic hiring data with bias"""
        genders = ['Male'] * (n_samples//2) + ['Female'] * (n_samples//2)
        np.random.shuffle(genders)

        ages = np.where(
            np.array(genders) == 'Male',
            np.random.normal(loc=38, scale=8, size=n_samples),
            np.random.normal(loc=35, scale=7, size=n_samples)
        ).clip(22, 60).astype(int)

        experiences = (ages - 22 + np.random.normal(scale=3, size=n_samples)).clip(0, 40).astype(int)

        hired = []
        for age, gender, exp in zip(ages, genders, experiences):
            prob = 0.3 + 0.5*(exp/20)
            if gender == 'Female': prob *= 0.5
            hired.append(int(np.random.random() < prob))

        return pd.DataFrame({
            'Age': ages,
            'Gender': genders,
            'Experience': experiences,
            'Hired': hired
        })

    def prepare_dataset(self, df):
        """Convert DataFrame to AIF360 dataset"""
        df = df.copy()
        df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})
        return StandardDataset(
            df=df,
            label_name='Hired',
            favorable_classes=[1],
            protected_attribute_names=['Gender'],
            privileged_classes=[[1]],
            instance_weights_name=None
        )

    def train(self, data=None):
        """Train the model with bias mitigation"""
        if data is None:
            data = self.generate_data()

        aif_data = self.prepare_dataset(data)

        # Bias analysis before mitigation
        print("\n[Pre-Training Bias Analysis]")
        self._analyze_bias(aif_data)

        # Apply reweighing
        rw = Reweighing(
            privileged_groups=self.privileged_groups,
            unprivileged_groups=self.unprivileged_groups
        )
        reweighted_data = rw.fit_transform(aif_data)

        # Split data
        train, test = reweighted_data.split([0.8], shuffle=True, seed=self.seed)

        # Scale features
        self.scaler = StandardScaler()
        X_train = self.scaler.fit_transform(train.features)
        y_train = train.labels.ravel()
        X_test = self.scaler.transform(test.features)
        y_test = test.labels.ravel()

        # Train model
        self.model = LogisticRegression()
        self.model.fit(X_train, y_train)

        # Evaluate
        print("\n[Training Results]")
        preds = self.model.predict(X_test)
        print("Accuracy:", accuracy_score(y_test, preds))
        print(classification_report(y_test, preds))

        # Post-training bias analysis
        aif_test = test.copy()
        aif_test.labels = preds.reshape(-1, 1)
        print("\n[Post-Training Bias Analysis]")
        self._analyze_bias(aif_test)

        return self.model

    def evaluate_new_data(self, new_data):
        """
        Evaluate new data (DataFrame or dict)
        Returns: {
            'predictions': array,
            'metrics': dict of fairness metrics,
            'classification_report': str
        }
        """
        if not isinstance(new_data, pd.DataFrame):
            new_data = pd.DataFrame([new_data])

        # Prepare dataset
        new_data_prepped = self.prepare_dataset(new_data)
        X = self.scaler.transform(new_data_prepped.features)
        y = new_data_prepped.labels.ravel()

        # Predict
        preds = self.model.predict(X)
        probs = self.model.predict_proba(X)[:, 1]

        # Calculate metrics
        new_data_prepped.labels = preds.reshape(-1, 1)
        metrics = self._calculate_metrics(new_data_prepped)

        return {
            'predictions': preds,
            'probabilities': probs,
            'metrics': metrics,
            'classification_report': classification_report(y, preds, output_dict=True)
        }

    def _analyze_bias(self, dataset):
        """Internal method for bias analysis"""
        metric = BinaryLabelDatasetMetric(
            dataset,
            unprivileged_groups=self.unprivileged_groups,
            privileged_groups=self.privileged_groups
        )
        print(f"Statistical Parity Difference: {metric.statistical_parity_difference():.4f}")
        print(f"Disparate Impact: {metric.disparate_impact():.4f}")

    def _calculate_metrics(self, dataset):
        """Calculate fairness metrics"""
        metric = BinaryLabelDatasetMetric(
            dataset,
            unprivileged_groups=self.unprivileged_groups,
            privileged_groups=self.privileged_groups
        )
        return {
            'statistical_parity_difference': metric.statistical_parity_difference(),
            'disparate_impact': metric.disparate_impact()
        }


# Example Usage
if __name__ == "__main__":
    # Initialize and train
    hiring_model = BiasAwareHiringModel()
    hiring_model.train()

    # Evaluate new candidates
    new_candidates = pd.DataFrame({
        'Age': [32, 45, 28],
        'Gender': ['Female', 'Male', 'Female'],
        'Experience': [8, 20, 5],
        'Hired': [1, 1, 0]  # Ground truth if available
    })

    results = hiring_model.evaluate_new_data(new_candidates)
    print("\n[New Data Evaluation]")
    print("Predictions:", results['predictions'])
    print("Probabilities:", results['probabilities'])
    print("Fairness Metrics:", results['metrics'])

"""**Testing Below** call func like this"""

# Initialize
model = BiasAwareHiringModel()

# Option 1: Train with generated data
model.train()

# Option 2: Train with custom data
# custom_data = pd.read_csv('hiring_data.csv')
# model.train(custom_data)

# Evaluate new candidates
new_applicant = {
    'Age': 35,
    'Gender': 'Female',
    'Experience': 10,
    'Hired': 0  # Optional ground truth
}
results = model.evaluate_new_data(new_applicant)